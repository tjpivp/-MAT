// future.js
export const FUTURE = {
  "signals": [
    "On-device LLMs becoming default features (privacy + latency).",
    "Multi-agent systems coordinating tasks with tool calling and RAG.",
    "Provenance & watermarking standards (C2PA-style) gaining adoption.",
    "Evaluation moving from benchmarks to task-specific, outcome-based tests.",
    "Smaller, specialized models outperforming giants on narrow tasks."
  ],
  "bets": [
    "By 2027\u20132029: Typical office suites ship with offline-capable models; retrieval-native apps become standard.",
    "Legal ops: Mandatory AI policies, provenance logs, and disclosure clauses in commercial contracts.",
    "Education: AI-first curricula; personalized tutors become normal; assessment shifts to oral + monitored practicals.",
    "Media: Synthetic-first production pipelines with human creative direction; provenance badges expected.",
    "Security: Prompt-injection/red-teaming become routine, with standardized hardening playbooks."
  ],
  "assumptions": [
    "Model scaling continues but compute becomes constrained; efficiency > sheer size.",
    "Open-source keeps pace in quality for many tasks; enterprises blend open + closed.",
    "Hybrid RAG (structured + unstructured data) beats pure LLM recall for reliability.",
    "Regulators prioritize transparency, safety documentation, and incident reporting over source code disclosure.",
    "Human-in-the-loop remains mandatory for high-stakes decisions (finance, health, law)."
  ]
}